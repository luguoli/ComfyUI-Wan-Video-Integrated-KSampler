import torch
import comfy.samplers
import comfy.model_management
import comfy.cldm.control_types
import node_helpers
from .cache import remove_cache
from .utils import *
from comfy.patcher_extension import CallbacksMP
from comfy.model_patcher import ModelPatcher
from comfy.model_base import WAN21
from tqdm import tqdm
from comfy.ldm.modules.attention import wrap_attn
import comfy.model_sampling
import copy


class WanVideoIntegratedKSampler:

    def __init__(self):
        self.device = comfy.model_management.intermediate_device()
    @classmethod
    def INPUT_TYPES(s):
        sageattn_modes = ["disabled", "auto", "sageattn_qk_int8_pv_fp16_cuda", "sageattn_qk_int8_pv_fp16_triton", "sageattn_qk_int8_pv_fp8_cuda", "sageattn_qk_int8_pv_fp8_cuda++", "sageattn3", "sageattn3_per_block_mean"]
        return {
            "required": {
                "model_high_noise": ("MODEL",),
                "model_low_noise": ("MODEL",),
                "clip": ("CLIP", ),
                "vae": ("VAE", {}),
                "positive_prompt": ("STRING", {"multiline": True, "dynamicPrompts": True, "placeholder": "æ­£å‘æç¤ºè¯ positive_prompt"}),
                "negative_prompt": ("STRING", {"multiline": True, "dynamicPrompts": True, "placeholder": "è´Ÿå‘æç¤ºè¯ negative_prompt", "default": "è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°"}),
                "batch_size": ("INT", {"default": 1, "min": 1, "max": 10}),
                "length": ("INT", {"default": 81, "min": 17, "max": 16384, "step": 4}),
                "width": ("INT", {"default": 720, "min": 8, "max": 16384, "step": 8}),
                "height": ("INT", {"default": 1280, "min": 8, "max": 16384, "step": 8}),
                "steps_high_noise": ("INT", {"default": 4, "min": 0, "max": 10000}),
                "cfg_high_noise": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 100.0, "step":0.1, "round": 0.01}),
                "steps_low_noise": ("INT", {"default": 4, "min": 0, "max": 10000}),
                "cfg_low_noise": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 100.0, "step":0.1, "round": 0.01}),
                "noise_seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff, "control_after_generate": True}),
                "sampler_name": (comfy.samplers.KSampler.SAMPLERS, ),
                "scheduler": (comfy.samplers.KSampler.SCHEDULERS, ),
            },
            "optional": {
                "start_image": ("IMAGE",),
                # "middle_image": ("IMAGE",),
                "end_image": ("IMAGE",),
                "clip_vision": ("CLIP_VISION",),
                "latent": ("LATENT", ),
                "torch_enable_fp16_accumulation": ("BOOLEAN", {"default": True, "tooltip": "Enable torch.backends.cuda.matmul.allow_fp16_accumulation, requires pytorch 2.7.0 nightly."}),
                "sage_attention": (sageattn_modes, {"default": "auto", "tooltip": "Global patch comfy attention to use sageattn, once patched to revert back to normal you would need to run this node again with disabled option."}),
                "wan_blocks_to_swap": ("INT", {"default": 0, "min": 0, "max": 40, "step": 1, "tooltip": "Number of transformer blocks to swap, the 14B model has 40, while the 1.3B model has 30 blocks"}),
                "sd3_shift": ("FLOAT", {"default": 5.0, "min": 0.0, "max": 100.0, "step":0.01}),
                "enable_clean_gpu_memory": ("BOOLEAN", {"default": False}),
                "enable_clean_cpu_memory_after_finish": ("BOOLEAN", {"default": False}),
                "enable_sound_notification": ("BOOLEAN", {"default": False}),
                # "middle_frame_ratio": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 1.0, "step": 0.01, "display": "slider",}),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID",
            },
        }

    RETURN_TYPES = ("IMAGE", "IMAGE", "LATENT")
    RETURN_NAMES = ("ç”Ÿæˆå›¾åƒåºåˆ—FrameImages", "æœ€åä¸€å¸§LastFrameImage", "ï¼ˆå¯é€‰ï¼‰Latent")
    FUNCTION = "sample"
    CATEGORY = "sampling"
    # æ³¨æ„è¯­è¨€æ–‡ä»¶ä¸­ä¸èƒ½ç”¨@ç¬¦å·
    DESCRIPTION = "ğŸ³ WanVideoè§†é¢‘é›†æˆé‡‡æ ·å™¨ - Ké‡‡æ ·å™¨ï¼Œè§†é¢‘ç”Ÿæˆé‡‡æ ·å™¨ï¼Œé«˜ä½å™ªé›†æˆï¼Œæ”¯æŒæ–‡ç”Ÿè§†é¢‘/å›¾ç”Ÿè§†é¢‘æ¨¡å¼ï¼Œæ”¯æŒé¦–å°¾å¸§ç”Ÿæˆè§†é¢‘ï¼Œæ‰¹é‡ç”Ÿæˆã€è‡ªåŠ¨æ˜¾å­˜/å†…å­˜ç®¡ç†ã€sageæ³¨æ„åŠ›ã€å—äº¤æ¢ã€SD3é‡‡æ ·ã€å£°éŸ³é€šçŸ¥ç­‰å…¨æ–¹ä½åŠŸèƒ½ï¼Œä¸éœ€è¦è¿é‚£ä¹ˆå¤šçº¿å•¦~~~~/ğŸ³ WanVideo Integrated KSampler - K-sampler for video generation with integrated high/low noise stages, supports text-to-video/image-to-video modes, supports generating videos with start/end frames, batch generation, automatic VRAM/RAM management, sage attention, block swapping, SD3 sampling, sound notifications and more comprehensive features, no need to connect so many wires~~~~ - Github: https://github.com/luguoli - ğŸ“§Email: luguoliï¹«vip.qq.com"


    def sample(self, model_high_noise, model_low_noise, clip, vae, positive_prompt, negative_prompt, batch_size, length, width, height, steps_high_noise, cfg_high_noise, steps_low_noise, cfg_low_noise, noise_seed, sampler_name, scheduler, start_image=None, middle_image=None, end_image=None, clip_vision=None, latent=None, torch_enable_fp16_accumulation=False, sage_attention="disabled", wan_blocks_to_swap=0, sd3_shift=0, enable_clean_gpu_memory=False, enable_clean_cpu_memory_after_finish=False, enable_sound_notification=False, middle_frame_ratio=0.5, unique_id=0):


        # æ£€æŸ¥åˆæ³•æ€§
        if width <= 0 or height <= 0:
            raise Exception("å®½åº¦å’Œé«˜åº¦å¿…é¡»å¤§äº 0 / Width and height must be greater than 0")

        if (steps_high_noise is None or steps_high_noise <= 0) and (steps_low_noise is None or steps_low_noise <= 0):
            raise Exception(f"é«˜å™ªæ­¥æ•°å’Œä½å™ªæ­¥æ•°ä¸èƒ½åŒæ—¶ä¸º0 / high_noise_steps and low_noise_steps cannot both be 0")

        # è‡ªåŠ¨è°ƒæ•´åˆ°åˆæ³•å€æ•°
        multiple = 8
        width = ((width + multiple - 1) // multiple) * multiple
        height = ((height + multiple - 1) // multiple) * multiple
        print(f"âš ï¸ è°ƒæ•´å°ºå¯¸ä¸º {width}x{height} / Adjusting size to {width}x{height}")

        model_cloned = False


        if torch_enable_fp16_accumulation:
            print("âœ¨ å¯ç”¨torch fp16ç´¯åŠ ")
            
            try:
                if not model_cloned:
                    model_high_noise = model_high_noise.clone()
                    model_low_noise = model_low_noise.clone()
                    model_cloned = True

                def patch_enable_fp16_accum(model):
                    torch.backends.cuda.matmul.allow_fp16_accumulation = True
                def patch_disable_fp16_accum(model):
                    torch.backends.cuda.matmul.allow_fp16_accumulation = False
                
                if torch_enable_fp16_accumulation:
                    if hasattr(torch.backends.cuda.matmul, "allow_fp16_accumulation"):
                        model_high_noise.add_callback(CallbacksMP.ON_PRE_RUN, patch_enable_fp16_accum)
                        model_high_noise.add_callback(CallbacksMP.ON_CLEANUP, patch_disable_fp16_accum)

                        model_low_noise.add_callback(CallbacksMP.ON_PRE_RUN, patch_enable_fp16_accum)
                        model_low_noise.add_callback(CallbacksMP.ON_CLEANUP, patch_disable_fp16_accum)
                    else:
                        raise RuntimeError("Failed to set fp16 accumulation, this requires pytorch 2.7.1 or higher")
                else:
                    if hasattr(torch.backends.cuda.matmul, "allow_fp16_accumulation"):
                        model_high_noise.add_callback(CallbacksMP.ON_PRE_RUN, patch_disable_fp16_accum)

                        model_low_noise.add_callback(CallbacksMP.ON_PRE_RUN, patch_disable_fp16_accum)
                    else:
                        raise RuntimeError("Failed to set fp16 accumulation, this requires pytorch 2.7.1 or higher")
            
                print("âœ… å¯ç”¨torch fp16ç´¯åŠ æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ å¯ç”¨torch fp16ç´¯åŠ å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ˜¯å¦å®‰è£…äº†pytorch 2.7.1æˆ–æ›´é«˜ç‰ˆæœ¬æˆ–å…³é—­æ­¤é¡¹è®¾ç½®")


        if sage_attention != "disabled":
            print("âœ¨ å¯ç”¨sageæ³¨æ„åŠ›")
            
            try:
                if not model_cloned:
                    model_high_noise = model_high_noise.clone()
                    model_low_noise = model_low_noise.clone()
                    model_cloned = True

                def get_sage_func(sage_attention, allow_compile=False):
                    print(f"Using sage attention mode: {sage_attention}")
                    from sageattention import sageattn
                    if sage_attention == "auto":
                        def sage_func(q, k, v, is_causal=False, attn_mask=None, tensor_layout="NHD"):
                            return sageattn(q, k, v, is_causal=is_causal, attn_mask=attn_mask, tensor_layout=tensor_layout)
                    elif sage_attention == "sageattn_qk_int8_pv_fp16_cuda":
                        from sageattention import sageattn_qk_int8_pv_fp16_cuda
                        def sage_func(q, k, v, is_causal=False, attn_mask=None, tensor_layout="NHD"):
                            return sageattn_qk_int8_pv_fp16_cuda(q, k, v, is_causal=is_causal, attn_mask=attn_mask, pv_accum_dtype="fp32", tensor_layout=tensor_layout)
                    elif sage_attention == "sageattn_qk_int8_pv_fp16_triton":
                        from sageattention import sageattn_qk_int8_pv_fp16_triton
                        def sage_func(q, k, v, is_causal=False, attn_mask=None, tensor_layout="NHD"):
                            return sageattn_qk_int8_pv_fp16_triton(q, k, v, is_causal=is_causal, attn_mask=attn_mask, tensor_layout=tensor_layout)
                    elif sage_attention == "sageattn_qk_int8_pv_fp8_cuda":
                        from sageattention import sageattn_qk_int8_pv_fp8_cuda
                        def sage_func(q, k, v, is_causal=False, attn_mask=None, tensor_layout="NHD"):
                            return sageattn_qk_int8_pv_fp8_cuda(q, k, v, is_causal=is_causal, attn_mask=attn_mask, pv_accum_dtype="fp32+fp32", tensor_layout=tensor_layout)
                    elif sage_attention == "sageattn_qk_int8_pv_fp8_cuda++":
                        from sageattention import sageattn_qk_int8_pv_fp8_cuda
                        def sage_func(q, k, v, is_causal=False, attn_mask=None, tensor_layout="NHD"):
                            return sageattn_qk_int8_pv_fp8_cuda(q, k, v, is_causal=is_causal, attn_mask=attn_mask, pv_accum_dtype="fp32+fp16", tensor_layout=tensor_layout)
                    elif "sageattn3" in sage_attention:
                        from sageattn3 import sageattn3_blackwell
                        if sage_attention == "sageattn3_per_block_mean":
                            def sage_func(q, k, v, is_causal=False, attn_mask=None, **kwargs):
                                return sageattn3_blackwell(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=is_causal, attn_mask=attn_mask, per_block_mean=True).transpose(1, 2)
                        else:
                            def sage_func(q, k, v, is_causal=False, attn_mask=None, **kwargs):
                                return sageattn3_blackwell(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=is_causal, attn_mask=attn_mask, per_block_mean=False).transpose(1, 2)

                    if not allow_compile:
                        sage_func = torch.compiler.disable()(sage_func)

                    @wrap_attn
                    def attention_sage(q, k, v, heads, mask=None, attn_precision=None, skip_reshape=False, skip_output_reshape=False, **kwargs):
                        if skip_reshape:
                            b, _, _, dim_head = q.shape
                            tensor_layout="HND"
                        else:
                            b, _, dim_head = q.shape
                            dim_head //= heads
                            q, k, v = map(
                                lambda t: t.view(b, -1, heads, dim_head),
                                (q, k, v),
                            )
                            tensor_layout="NHD"
                        if mask is not None:
                            # add a batch dimension if there isn't already one
                            if mask.ndim == 2:
                                mask = mask.unsqueeze(0)
                            # add a heads dimension if there isn't already one
                            if mask.ndim == 3:
                                mask = mask.unsqueeze(1)
                        out = sage_func(q, k, v, attn_mask=mask, is_causal=False, tensor_layout=tensor_layout)
                        if tensor_layout == "HND":
                            if not skip_output_reshape:
                                out = (
                                    out.transpose(1, 2).reshape(b, -1, heads * dim_head)
                                )
                        else:
                            if skip_output_reshape:
                                out = out.transpose(1, 2)
                            else:
                                out = out.reshape(b, -1, heads * dim_head)
                        return out
                    return attention_sage

                new_attention = get_sage_func(sage_attention, allow_compile=False)

                def attention_override_sage(func, *args, **kwargs):
                    return new_attention.__wrapped__(*args, **kwargs)

                # attention override
                model_high_noise.model_options["transformer_options"]["optimized_attention_override"] = attention_override_sage
                model_low_noise.model_options["transformer_options"]["optimized_attention_override"] = attention_override_sage

                print(f"âœ… åº”ç”¨sageæ³¨æ„åŠ›æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ å¯ç”¨sageæ³¨æ„åŠ›å¤±è´¥ï¼Œè¯·ç¦ç”¨æ­¤é¡¹è®¾ç½®")


        
        if wan_blocks_to_swap > 0:
            print(f"âœ¨ åº”ç”¨å—äº¤æ¢")

            try:
                if not model_cloned:
                    model_high_noise = model_high_noise.clone()
                    model_low_noise = model_low_noise.clone()
                    model_cloned = True

                offload_img_emb = False
                offload_txt_emb = False
                use_non_blocking = False
                def swap_blocks(model_instance: ModelPatcher, device_to, lowvram_model_memory, force_patch_weights, full_load):
                    base_model = model_instance.model
                    main_device=torch.device('cuda')

                    if not isinstance(base_model, WAN21):
                        raise TypeError("swap_blocks only supports WAN21 models")
                    
                    unet = base_model.diffusion_model
                    num_blocks = len(unet.blocks)
                    swap_count = min(wan_blocks_to_swap, num_blocks)

                    if offload_txt_emb:
                        unet.text_embedding.to(model_instance.offload_device, non_blocking=use_non_blocking)
                    if offload_img_emb:
                        unet.img_emb.to(model_instance.offload_device, non_blocking=use_non_blocking)

                    with tqdm(total=num_blocks, desc="Initializing block swap", leave=True) as pbar:
                        for idx, block in enumerate(unet.blocks):
                            if idx < swap_count:
                                # ä½ idx çš„ block æ”¾åˆ° offload_device
                                block.to(model_instance.offload_device)
                            else:
                                # å…¶ä»– block æ”¾å› GPU
                                block.to(main_device)
                            pbar.update(1)

                    comfy.model_management.soft_empty_cache()
                    gc.collect()
                
                model_high_noise.add_callback(CallbacksMP.ON_LOAD,swap_blocks)
                model_low_noise.add_callback(CallbacksMP.ON_LOAD,swap_blocks)

                print("âœ… å—äº¤æ¢å‚æ•°å·²åº”ç”¨æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ å—äº¤æ¢å¤±è´¥ï¼Œè¯·å…³é—­æ­¤é¡¹è®¾ç½®")


        if sd3_shift > 0:
            print(f"âœ¨ åº”ç”¨é‡‡æ ·ç®—æ³•ï¼ˆSD3ï¼‰")

            try:
                if not model_cloned:
                    model_high_noise = model_high_noise.clone()
                    model_low_noise = model_low_noise.clone()
                    model_cloned = True

                sampling_base = comfy.model_sampling.ModelSamplingDiscreteFlow
                sampling_type = comfy.model_sampling.CONST
                class ModelSamplingAdvanced(sampling_base, sampling_type):
                    pass

                model_sampling = ModelSamplingAdvanced(model_high_noise.model.model_config)
                model_sampling.set_parameters(shift=sd3_shift, multiplier=1000)
                model_high_noise.add_object_patch("model_sampling", model_sampling)

                model_sampling = ModelSamplingAdvanced(model_low_noise.model.model_config)
                model_sampling.set_parameters(shift=sd3_shift, multiplier=1000)
                model_low_noise.add_object_patch("model_sampling", model_sampling)

                print("âœ… é‡‡æ ·ç®—æ³•ï¼ˆSD3ï¼‰å·²åº”ç”¨æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ é‡‡æ ·ç®—æ³•ï¼ˆSD3ï¼‰åº”ç”¨å¤±è´¥ï¼Œè¯·å…³é—­æ­¤é¡¹è®¾ç½®")


        models = (model_high_noise, model_low_noise)
        steps = (steps_high_noise, steps_low_noise)
        cfgs = (cfg_high_noise, cfg_low_noise)
        disable_noises = (False, True)
        force_full_denoises = (False, True)

        
        with tqdm(total=4, desc="CLIP Encoding Progress") as pbar:
            # åŠ è½½æ­£å‘æ¡ä»¶
            positive_tokens = clip.tokenize(positive_prompt)
            pbar.update(1)
            positive = clip.encode_from_tokens_scheduled(positive_tokens)
            pbar.update(1)
            # åŠ è½½è´Ÿå‘æ¡ä»¶
            negative_tokens = clip.tokenize(negative_prompt)
            pbar.update(1)
            negative = clip.encode_from_tokens_scheduled(negative_tokens)
            pbar.update(1)


        positive_high_noise = copy.deepcopy(positive)
        negative_high_noise = copy.deepcopy(negative)

        positive_low_noise = copy.deepcopy(positive)
        negative_low_noise = copy.deepcopy(negative)


        if latent is None:

            if all(x is None for x in [start_image, middle_image, end_image]):
                latent_image = torch.zeros([batch_size, 16, ((length - 1) // 4) + 1, height // 8, width // 8], device=self.device)
                latent = {"samples":latent_image}
                print("æ–‡ç”Ÿè§†é¢‘æ¨¡å¼")
            else:

                # å°¾å¸§æ£€æŸ¥
                if end_image is not None and start_image is None:
                    raise Exception("ä½¿ç”¨å°¾å¸§æ—¶å¿…é¡»ä¼ å…¥é¦–å¸§ / When using end_image, start_image must also be provided.")
                
                # ä¸­é—´å¸§æ£€æŸ¥
                if middle_image is not None and (start_image is None or end_image is None):
                    raise Exception("ä½¿ç”¨ä¸­é—´å¸§æ—¶å¿…é¡»ä¼ å…¥é¦–å°¾å¸§ / When using middle_image, both start_image and end_image must be provided.")
    

                spacial_scale = vae.spacial_compression_encode()
                latent_channels = vae.latent_channels
                latent_t = ((length - 1) // 4) + 1
                latent_image = torch.zeros([batch_size, latent_channels, latent_t, height // spacial_scale, width // spacial_scale], device=self.device)

                if start_image is not None:
                    start_image, resize_width, resize_height, resize_mask = image_resize(start_image, width, height, "crop", "lanczos", 2, "0, 0, 0", "center", unique_id=unique_id, device="cpu", mask=None, per_batch=64)
                    start_image = comfy.utils.common_upscale(start_image[:length].movedim(-1, 1), resize_width, resize_height, "bilinear", "center").movedim(1, -1)
                if middle_image is not None:
                    middle_image, resize_width, resize_height, resize_mask = image_resize(middle_image, width, height, "crop", "lanczos", 2, "0, 0, 0", "center", unique_id=unique_id, device="cpu", mask=None, per_batch=64)
                    middle_image = comfy.utils.common_upscale(middle_image[-length:].movedim(-1, 1), width, height, "bilinear", "center").movedim(1, -1)
                if end_image is not None:
                    end_image, resize_width, resize_height, resize_mask = image_resize(end_image, width, height, "crop", "lanczos", 2, "0, 0, 0", "center", unique_id=unique_id, device="cpu", mask=None, per_batch=64)
                    end_image = comfy.utils.common_upscale(end_image[-length:].movedim(-1, 1), width, height, "bilinear", "center").movedim(1, -1)

                image = torch.ones((length, height, width, 3), device=self.device) * 0.5
                mask = torch.ones((1, 1, latent_image.shape[2] * 4, latent_image.shape[-2], latent_image.shape[-1]), device=self.device)

                image_high_noise = image.clone()
                image_low_noise = image.clone()

                mask_high_noise = mask.clone()
                mask_low_noise = mask.clone()


                middle_idx = calculate_middle_frame_idx(middle_frame_ratio, length)

                if start_image is not None:
                    image_high_noise[:start_image.shape[0]] = start_image
                    image_low_noise[:start_image.shape[0]] = start_image
                    mask_high_noise[:, :, :start_image.shape[0] + 3] = 0.0
                    mask_low_noise[:, :, :start_image.shape[0] + 3] = 0.0

                if middle_image is not None:
                    # TODO:ä¸­é—´å¸§é—ªçƒé—®é¢˜æš‚æœªè§£å†³
                    n_middle = middle_image.shape[0]
                    image_high_noise[middle_idx:middle_idx + n_middle] = middle_image
                    mask_high_noise[:, :, middle_idx:middle_idx + n_middle + 3] = 0.05
                    
                if end_image is not None:
                    image_high_noise[-end_image.shape[0]:] = end_image
                    image_low_noise[-end_image.shape[0]:] = end_image
                    mask_high_noise[:, :, -end_image.shape[0]:] = 0.0
                    mask_low_noise[:, :, -end_image.shape[0]:] = 0.1

                concat_latent_image_high_noise = vae.encode(image_high_noise[:, :, :, :3])
                concat_latent_image_low_noise = vae.encode(image_low_noise[:, :, :, :3])

                mask_high_noise = mask_high_noise.view(1, mask_high_noise.shape[2] // 4, 4, mask_high_noise.shape[3], mask_high_noise.shape[4]).transpose(1, 2)
                mask_low_noise = mask_low_noise.view(1, mask_low_noise.shape[2] // 4, 4, mask_low_noise.shape[3], mask_low_noise.shape[4]).transpose(1, 2)

                positive_high_noise = node_helpers.conditioning_set_values(positive_high_noise, {"concat_latent_image": concat_latent_image_high_noise, "concat_mask": mask_high_noise})
                negative_high_noise = node_helpers.conditioning_set_values(negative_high_noise, {"concat_latent_image": concat_latent_image_high_noise, "concat_mask": mask_high_noise})

                positive_low_noise = node_helpers.conditioning_set_values(positive_low_noise, {"concat_latent_image": concat_latent_image_low_noise, "concat_mask": mask_low_noise})
                negative_low_noise = node_helpers.conditioning_set_values(negative_low_noise, {"concat_latent_image": concat_latent_image_low_noise, "concat_mask": mask_low_noise})

                

                clip_vision_list = []

                if clip_vision is not None:
                    if start_image is not None:
                        #clip_visionç¼–ç 
                        clip_vision_encode_start_image = clip_vision.encode_image(start_image, crop=False)
                        clip_vision_list.append(clip_vision_encode_start_image)

                    if middle_image is not None:
                        #clip_visionç¼–ç 
                        clip_vision_encode_middle_image = clip_vision.encode_image(middle_image, crop=False)
                        clip_vision_list.append(clip_vision_encode_middle_image)

                    if end_image is not None:
                        #clip_visionç¼–ç 
                        clip_vision_encode_end_image = clip_vision.encode_image(end_image, crop=False)
                        clip_vision_list.append(clip_vision_encode_end_image)

                clip_vision_output = None
                if clip_vision_list:  # åˆ—è¡¨éç©º
                    states = torch.cat([c.penultimate_hidden_states for c in clip_vision_list], dim=-2)
                    clip_vision_output = comfy.clip_vision.Output()
                    clip_vision_output.penultimate_hidden_states = states

                # åº”ç”¨åˆ°æ­£/è´Ÿæ¡ä»¶
                if clip_vision_output is not None:
                    positive_high_noise = node_helpers.conditioning_set_values(positive_high_noise, {"clip_vision_output": clip_vision_output})
                    negative_high_noise = node_helpers.conditioning_set_values(negative_high_noise, {"clip_vision_output": clip_vision_output})

                    positive_low_noise = node_helpers.conditioning_set_values(positive_low_noise, {"clip_vision_output": clip_vision_output})
                    negative_low_noise = node_helpers.conditioning_set_values(negative_low_noise, {"clip_vision_output": clip_vision_output})

                latent = {"samples":latent_image}

        positive = (positive_high_noise, positive_low_noise)
        negative = (negative_high_noise, negative_low_noise)


        print("ğŸš€ å¼€å§‹é‡‡æ ·è¿‡ç¨‹/Starting Sampling...")

        if enable_clean_gpu_memory:
            print("ğŸ—‘ï¸ é¢„æ¸…ç†æ˜¾å­˜å ç”¨/Pre-cleaning GPU memory...")
            try:
                cleanGPUUsedForce()
                remove_cache('*')
            except ImportError:
                print("ğŸ”• æ˜¾å­˜æ¸…ç†å¤±è´¥/Pre GPU memory cleaning failed")
            print("âœ… é¢„æ˜¾å­˜æ¸…ç†å®Œæˆ/Pre GPU memory cleaning completed")


        latent_output = common_ksampler(models, noise_seed, steps, cfgs, sampler_name, scheduler, positive, negative, latent, denoise=1.0, disable_noises=disable_noises, force_full_denoises=force_full_denoises)

        print("ğŸ–¼ï¸ æ­£åœ¨è§£ç æ½œç©ºé—´/Decoding latent space...")
        output_images = vae.decode(latent_output["samples"])
        if len(output_images.shape) == 5: #Combine batches
            output_images = output_images.reshape(-1, output_images.shape[-3], output_images.shape[-2], output_images.shape[-1])
        print("âœ… è§£ç å®Œæˆ/Decoding completed")

        # æŠ½å–æœ€åä¸€å¸§ï¼Œå–å¸§ç¤ºä¾‹ï¼š[1, 2, 3, 4, -1]
        index_list = [-1]
        # Convert list of indices to a PyTorch tensor
        indices_tensor = torch.tensor(index_list, dtype=torch.long)
        # Select the images at the specified indices
        last_image = output_images[indices_tensor]


        if enable_clean_gpu_memory:
            print("ğŸ—‘ï¸ åæ¸…ç†æ˜¾å­˜å ç”¨/Post-cleaning GPU memory...")
            try:
                cleanGPUUsedForce()
                remove_cache('*')
            except ImportError:
                print("ğŸ”• æ˜¾å­˜æ¸…ç†å¤±è´¥/Pre GPU memory cleaning failed")
            print("âœ… åæ˜¾å­˜æ¸…ç†å®Œæˆ/Post GPU memory cleaning completed")

        if enable_clean_cpu_memory_after_finish:
            print("ğŸ—‘ï¸ å®Œæˆåæ¸…ç†CPUå†…å­˜/Post-cleaning CPU memory after finish...")
            try:
                clean_ram(clean_file_cache=True, clean_processes=True, clean_dlls=True, retry_times=3)
            except Exception as e:
                print(f"ğŸ”• RAMæ¸…ç†å¤±è´¥/RAM cleanup failed: {str(e)}")
            else:
                print("âœ… [Clean CPU Memory After Finish] RAMæ¸…ç†å®Œæˆ / RAM cleanup completed")

        if enable_sound_notification:
            try:
                import winsound
                import time
                # æ’­æ”¾å¿«é€Ÿç´§å‡‘çš„æ—‹å¾‹ï¼šA4, C5, E5, G5ï¼Œè¾ƒçŸ­é—´éš”ä½¿æ—‹å¾‹è¿è´¯
                frequencies = [440, 523, 659, 784]
                for freq in frequencies:
                    winsound.Beep(freq, 150)
                    time.sleep(0.005)  # æ›´çŸ­é—´éš”åŠ å¿«èŠ‚å¥
                print("ğŸµ [Sound Notification] Completion melody played")
            except ImportError:
                print("ğŸ”• [Sound Notification] Sound notification not supported on this system")
            except Exception as e:
                print(f"ğŸ”• [Sound Notification] Audio playback failed: {str(e)}")

        return (output_images, last_image, latent_output)



class WanVideoIntegratedKSamplerSimple:

    def __init__(self):
        pass
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "model_high_noise": ("MODEL",),
                "model_low_noise": ("MODEL",),
                "steps_high_noise": ("INT", {"default": 4, "min": 0, "max": 10000}),
                "cfg_high_noise": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 100.0, "step":0.1, "round": 0.01}),
                "steps_low_noise": ("INT", {"default": 4, "min": 0, "max": 10000}),
                "cfg_low_noise": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 100.0, "step":0.1, "round": 0.01}),
                "noise_seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff, "control_after_generate": True}),
                "sampler_name": (comfy.samplers.KSampler.SAMPLERS, ),
                "scheduler": (comfy.samplers.KSampler.SCHEDULERS, ),
                "positive": ("CONDITIONING", ),
                "negative": ("CONDITIONING", ),
                "latent": ("LATENT", ),
            },
        }

    RETURN_TYPES = ("LATENT",)
    RETURN_NAMES = ("Latent",)
    FUNCTION = "sample"
    CATEGORY = "sampling"
    # æ³¨æ„è¯­è¨€æ–‡ä»¶ä¸­ä¸èƒ½ç”¨@ç¬¦å·
    DESCRIPTION = "ğŸ³ WanVideoè§†é¢‘é›†æˆé‡‡æ ·å™¨(ç®€å•)â€”â€”Github:@luguoli"


    def sample(self, model_high_noise, model_low_noise, steps_high_noise, cfg_high_noise, steps_low_noise, cfg_low_noise, noise_seed, sampler_name, scheduler, positive, negative, latent):
        models = (model_high_noise, model_low_noise)
        steps = (steps_high_noise, steps_low_noise)
        cfgs = (cfg_high_noise, cfg_low_noise)
        positive = (positive, positive)
        negative = (negative, negative)
        disable_noises = (False, True)
        force_full_denoises = (False, True)
        latent_output = common_ksampler(models, noise_seed, steps, cfgs, sampler_name, scheduler, positive, negative, latent, denoise=1.0, disable_noises=disable_noises, force_full_denoises=force_full_denoises)

        return (latent_output,)

NODE_CLASS_MAPPINGS = {
    "WanVideoIntegratedKSampler": WanVideoIntegratedKSampler,
    "WanVideoIntegratedKSamplerSimple": WanVideoIntegratedKSamplerSimple,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "WanVideoIntegratedKSampler": "ğŸ³ WanVideoè§†é¢‘é›†æˆé‡‡æ ·å™¨â€”â€”Github:@luguoli",
    "WanVideoIntegratedKSamplerSimple": "ğŸ³ WanVideoè§†é¢‘é›†æˆé‡‡æ ·å™¨(ç®€å•)â€”â€”Github:@luguoli",
}
